一元线性回归分析：回归分析中包括一个自变量和一个因变量，且两者的关系可用一条直线近似表示。

多元线性回归分析：回归分析中包括多个自变量，且自变量和因变量的关系可用一条直线近似表示。

## 线性回归模型：

$$
y=wx+b
$$
假设（预测）函数——h(x)

损失（误差）函数——L(w, b)

```python
# 预测网点销售额

# 导入数据，构建特征集合 X 和标签集 y
import numpy as np #导入NumPy数学工具箱
import pandas as pd #导入Pandas数据处理工具箱
df_ads = pd.read_csv('../input/advertisingsimpledataset/advertising.csv')
X = np.array(df_ads.wechat) #构建特征集，只含有微信广告一个特征
y = np.array(df_ads.sales) #构建标签集，销售金额

# 机器学习模型所读入的规范格式是 2D 张量，通过reshape函数把向量转换为矩阵
X = X.reshape((len(X),1)) 
y = y.reshape((len(y),1))

# 将数据集拆分为训练集和测试集
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# 定义归一化函数，并将标签和特征归一化
def scaler(train, test):   
    min = train.min(axis=0) 
    max = train.max(axis=0) 
    gap = max - min 
    train -= min 
    train /= gap 
    test -= min 
    test /= gap 
    return train, test 
X_train,X_test = scaler(X_train,X_test) 
y_train,y_test = scaler(y_train,y_test) 

# 定义 MSE 均方误差函数（损失函数）
def loss_function(X, y, weight, bias): 
    y_hat = weight*X + bias 
    loss = y_hat-y  
    cost = np.sum(loss**2)/(2*len(X)) 
    return cost 

# 定义梯度下降函数
def gradient_descent(X, y, w, b, lr, iter): 
    l_history = np.zeros(iter) # 初始化记录梯度下降过程中损失的数组
    w_history = np.zeros(iter) # 初始化记录梯度下降过程中权重的数组
    b_history = np.zeros(iter) # 初始化记录梯度下降过程中偏置的数组
    for i in range(iter): 
        y_hat  = w*X + b 
        loss = y_hat-y 
        derivative_w = X.T.dot(loss)/len(X) # 对权重 w 求导
        derivative_b = sum(loss)*1/len(X)   # 对偏置 b 求导
        w = w - lr*derivative_w # 结合下降速率alpha更新权重 w 
        b = b - lr*derivative_b # 结合下降速率alpha更新偏置 b
        l_history[i] = loss_function(X, y, w,b) # 梯度下降过程中损失的历史
        w_history[i] = w # 梯度下降过程中权重的历史
        b_history[i] = b # 梯度下降过程中偏置的历史
    return l_history, w_history, b_history 

# 确定参数的初始值
iterations = 225; # 迭代250次
weight = -5 # 权重
bias = 3 # 偏置
alpha = 0.5; # 此处初始学习速率设为0.5， 如果调整为1，你会看到不同的结果

# 根据初始参数值，进行梯度下降，也就是开始训练机器，拟合函数
loss_history, weight_history, bias_history = gradient_descent(X_train, y_train, weight, bias, alpha, iterations)

# 结果
print ('当前损失：',loss_function(X_train, y_train, weight_history[-1], bias_history[-1]))
print ('当前权重：',weight_history[-1])
print ('当前偏置：',bias_history[-1])

# 测试集结果
print ('测试集损失：',loss_function(X_test, y_test, weight_history[-1], bias_history[-1]))
```



## 多元线性回归模型：

$$
y = b + w_1x_1 + w_2x_2 + w_3x_3
$$

若将 b 也看作权重，则可将公式变换为：
$$
y = w_0x_0 + w_1x_1 + w_2x_2 + w_3x_3
$$

```python
# 示例：线性回归模型预测加州房价

# 导入数据，构建特征集合 X 和标签集 y
import pandas as pd #导入Pandas，用于数据读取和处理
df_housing = pd.read_csv("../input/housecsv/house.csv") 
X = df_housing.drop("median_house_value",axis = 1)  
y = df_housing.median_house_value                 

# 将数据集拆分为训练集和测试集
from sklearn.model_selection import train_test_split 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0) 

# 用线性回归模型训练机器
from sklearn.linear_model import LinearRegression 
model = LinearRegression() 
model.fit(X_train, y_train) 

# 测试结果
y_pred = model.predict(X_test) 
print ('房价的真值(测试集)',y_test)
print ('预测的房价(测试集)',y_pred)
print("给预测评分：", model.score(X_test, y_test))
```

